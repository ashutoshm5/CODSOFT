# -*- coding: utf-8 -*-
"""CUSTOMER CHURN  PREDICTION

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QVpfmordQs-NTe33EvWUP_rtGzSLgv-A
"""



display(df.info())
display(df.head())

"""### Compare and select best model

Based on the evaluation metrics, let's compare the performance of the models and select the best one.

Looking at the results:

*   **Logistic Regression:** Has the lowest performance across all metrics.
*   **Random Forest:** Shows significantly better performance than Logistic Regression, particularly in Precision and ROC AUC.
*   **Gradient Boosting:** Performs slightly better than Random Forest in terms of Accuracy and ROC AUC, and has a higher Recall, although its Precision is slightly lower.

Considering the trade-off between Precision and Recall, and the overall higher ROC AUC, **Gradient Boosting** appears to be the best performing model for this churn prediction task among the ones we've trained.

### Finish task
The analysis shows that the Gradient Boosting model is the most effective among the tested algorithms for predicting customer churn based on the provided dataset and features.
"""

# Evaluate models
models = {
    "Logistic Regression": log_reg,
    "Random Forest": random_forest,
    "Gradient Boosting": gradient_boosting
}

for name, model in models.items():
    y_pred = model.predict(X_test_scaled)
    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_pred_proba)

    print(f"--- {name} ---")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1-Score: {f1:.4f}")
    print(f"ROC AUC: {roc_auc:.4f}")
    print("-" * (len(name) + 6))

"""### Evaluate model

Evaluate the performance of the trained models on the testing data using appropriate metrics.
"""

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.preprocessing import StandardScaler

# Scale the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


# Initialize models
log_reg = LogisticRegression(random_state=42)
random_forest = RandomForestClassifier(random_state=42)
gradient_boosting = GradientBoostingClassifier(random_state=42)

# Train models on scaled data
log_reg.fit(X_train_scaled, y_train)
random_forest.fit(X_train_scaled, y_train)
gradient_boosting.fit(X_train_scaled, y_train)


print("Models trained successfully on scaled data!")

"""### Train model

Train different classification models (Logistic Regression, Random Forests, Gradient Boosting) on the training data.
"""

from sklearn.model_selection import train_test_split

# Separate features (X) and target variable (y)
X = df.drop('Exited', axis=1)
y = df['Exited']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Training set shape:", X_train.shape)
print("Testing set shape:", X_test.shape)

"""### Split data

Split the dataset into training and testing sets.
"""

# Drop irrelevant columns
df = df.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)

# One-hot encode categorical features
df = pd.get_dummies(df, columns=['Geography', 'Gender'], drop_first=True)

display(df.head())

"""### Feature Engineering

Create new features or transform existing ones to improve the performance of our churn prediction model.

### Explore and preprocess data

This step involves understanding the structure of the data, handling missing values, and preprocessing the features.
"""